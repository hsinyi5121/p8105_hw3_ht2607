---
title: "p8105_hw3_ht2607"
output: github_document
---

## Setting up 

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Question 1

This problem is using the Instacart data. However, I did not download the dataset into my local data directory; instead, I loaded the data from the p8105.datasets using the below code.
```{r}
library(p8105.datasets)

data("instacart")

instacart = 
  instacart |> 
  as_tibble()
```

In the "instacart" dataset, there is total 1384617 observation and 15 variables. Key variables such as order_id (numeric), aisle_id (numeric), aisel (character), product_id (numeric), order_number (numeric), product_name (character), and order_hour_of_day  (numeric). 

1. How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart |> 
  count(aisle) |> 
  arrange(desc(n))
```
There are total 134 aisles, and the fresh vegetable and fresh fruits are the aisles that are the most items ordered form. 

2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
Next is a plot that shows the number of items ordered in each aisle. Here, aisles are   ordered by ascending number of items.

```{r}
instacart |> 
  count(aisle) |> 
  filter(n > 10000) |> 
  mutate(aisle = fct_reorder(aisle, n)) |> 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |>
  group_by(aisle) |> 
  count(product_name) |> 
  mutate(rank = min_rank(desc(n))) |> 
  filter(rank < 4) |> 
  arrange(desc(n)) |>
  knitr::kable()
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  group_by(product_name, order_dow) |>
  summarize(mean_hour = mean(order_hour_of_day)) |>
  pivot_wider(
    names_from = order_dow, 
    values_from = mean_hour) |>
  knitr::kable(digits = 2)
```

## Question 2

Step 1: loading the dataset from the p8105 dataset package
```{r}
library(tidyverse)
library(p8105.datasets)
data("brfss_smart2010")
```

Step 2: format the data to use appropriate variable names using clean name function

```{r}
brfss_smart2010 = janitor::clean_names(brfss_smart2010)
names(brfss_smart2010)
```

Step 3: focusing on the “Overall Health” topic

```{r}
overall_health_df=
  filter(brfss_smart2010, topic == "Overall Health")
```

Step 4: include only responses from “Excellent” to “Poor”

```{r}
overall_health_ep=
  filter(overall_health_df, response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") |>
  mutate(name = forcats::fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor")))
```

Step 5: organize responses as a factor taking levels ordered from “Poor” to “Excellent”

```{r}
overall_health_ep |> 
  mutate(nameback = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

Answering questions:
In 2002, which states were observed at 7 or more locations? What about in 2010?

In this code chuck, I filtered the dataframe to 2002, and group by the locationabbr variable. Then, I summarizeed to compute multiple summaries within each group. Last, I filter to states that were observed at 7 or more locations. 

```{r}
q1 =
  filter(overall_health_ep, year == "2002") |> 
  group_by(locationabbr) |> 
  summarize(loc = n_distinct(locationdesc)) |> 
  filter(loc >= 7)

q1
```
In 2002, states that were observed at 7 or more locations are CT, FL, MA, NC, NJ, and PA.

In this code chuck, I did the similar step as I described for 2002.
```{r}
q1_b =
  filter(overall_health_ep, year == "2010") |> 
  group_by(locationabbr) |> 
  summarize(loc = n_distinct(locationdesc)) |> 
  filter(loc >= 7)

q1_b
```
In 2002, states that were observed at 7 or more locations are CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA. 

Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state.

```{r}
q1_c=
  select(overall_health_ep, year, locationabbr, response, data_value) |> 
  filter(response == "Excellent")
```

Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
q1_c |> 
  group_by(year, locationabbr) |> 
  summarize(avg_data_value = mean(data_value)) |> 
  ggplot(aes(x = year, y = avg_data_value, color = locationabbr)) +
  geom_line(aes(group = locationabbr)) +
  labs(
    x = "Year",
    y = "Average Data Value",
    title = "Spaghetti Plot of Average Data Value by State Over Time"
  ) +
   theme(legend.position = "bottom")
```
The spaghetti plot shows that there is not much different by States' average data value over time. However, it looks like WY has a dramatic change from 2004 to 2010. 

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
library(ggplot2)
q1_d=
  select(overall_health_df, year, locationabbr, response, data_value) |> 
  filter(locationabbr == "NY", year == 2006 | year == 2010) |> 
  group_by(locationabbr, year) |> 
  ggplot(aes(x = response, y = data_value, color = response)) +
  geom_point() + geom_line() +
  geom_boxplot() +
  facet_grid(. ~ year, scales = "free_y")
  labs(
    x = "Response",
    y = "Data Value",
    title = "Distribution of Data Value by Response in New York State (2006 and 2010)"
  ) +
   theme(legend.position = "bottom")

q1_d
```
From the two panel plot, we can see that majority of the responses for the services from 2006 to 2010 are fall under excellent, very good, and good. However, we do see that there are less than 20 of data value that responses fair in 2006. In addition, there is less than 10 data values that have response of poor in both 2006 and 2010. 

## Question 3

Step 1: imported and loaded both csv files. Also, tidy, merge, and otherwise organize the data sets. Your final dataset should include all originally observed variables; exclude participants less than 21 years of age, and those with missing demographic data. In addition, encode data with reasonable variable classes (i.e. not numeric, and using factors with the ordering of tables and plots in mind). 

```{r}
library(tidyverse)
accel = read_csv(file = "./q3data/nhanes_accel.csv", na = "NA") |>
  janitor::clean_names() |> 
  pivot_longer(
    min1:min1440,
    names_to = "minutes", 
    values_to = "mins")
    
covar = read_csv(file = "./q3data/nhanes_covar.csv", skip = 4, na = "NA") |>
  janitor::clean_names() |> 
  filter(age >= 21 | is.na(age)) |> 
  drop_na() |> 
  mutate(
    sex = 
      case_match(
        sex, 
        1 ~ "male", 
        2 ~ "female"),
    sex = as.factor(sex)) |> 
  mutate(
    education = 
      case_match(
        education, 
        1 ~ "less than high school", 
        2 ~ "high school equivalent", 
        3 ~ "more than high school"),
    education = as.factor(education)) |> 
   mutate(school = forcats::fct_relevel(education, c("less than high school", "high school equivalent", "more than high school")))

q3tidy_df = 
  inner_join(covar, accel, by = "seqn")
```

Step 2: 
Produce a reader-friendly table for the number of men and women in each education category

```{r}
covar |> 
  count(sex, school) |> 
  pivot_wider(
  names_from = sex, 
  values_from = n) |> 
  janitor::clean_names() |> 
knitr::kable()
```
We can see that majority of the numbers of men and women are fall within the more than high school education category. Also, there are more males compared to females in the  high school equivalent category. Lastly, in the less than high school category, the numbers of female and male are relatively the same. 

Step 3: create a visualization of the age distributions for men and women in each education category

```{r}
covar |> 
 ggplot(aes(x = age, fill = sex)) +
  geom_density(alpha = .5) + 
  facet_grid(. ~ education) + 
  labs(
    x = "Age",
    y = "Count",
    title = "Age distributions for men and women in each education category"
  ) 
  viridis::scale_fill_viridis(discrete = TRUE)
```
In the visualization of the age distribution for men and women in each education category, we can see that more females that is less than age of 50 are in the more than high school education category. It is obvious that the peak of the plot in the more than high school category is much higher among the mid 30 female. Also, we can see that there is more females that age more than 60 are in the high school equivalent education category. Additionally, there is more males that age less than 50 are in the high school equivalent education category. 

Plot these total activities (y-axis) against age (x-axis); your plot should compare men to women and have separate panels for each education level. Include a trend line or a smooth to illustrate differences. 
```{r}
q3tidy_df|> 
  group_by(education, sex, age) |> 
  summarise(Total_Activity = sum(mins)) |> 
  ggplot(aes(x = age, y = Total_Activity, color = sex)) + 
    geom_point(alpha = .5) +
    geom_smooth(se = FALSE) + 
    facet_grid(. ~ education) +
    labs(
      x = "Age",
      y = "Total Activity",
      title = "Total Activity vs. Age by Education Level and Gender"
   )
    theme(legend.position = "bottom")
```
In the "Total activity vs age by education level and gender" graph, we can see the among the high school equivalent education group, the trend of total activity has increase, specifically from the age 70 and above. However, in the less than high school education group, the trend of total activity decrease particularly for male that is age 60 and above. However, there are higher total activity among those who is under age of 50 in the more than high school education group. 

Make a three-panel plot that shows the 24-hour activity time courses for each education level and use color to indicate sex.

```{r}
q3tidy_df |> 
  ggplot(aes(x = minutes, y = mins, color = sex)) +
    geom_point(alpha = .5) +
  geom_smooth(se = FALSE) +
    facet_grid(. ~ education) +
    labs(
      x = "Minute of the Day",
      y = "mins",
      title = "24-Hour Activity Time Courses by Education Level and Sex"
    ) +
   theme(legend.position = "bottom")

```
Based on this graph, we can conclude that there are more males in the high school equivalent education category compared to females in the 24 hours activity time courses. However, more females compared to males in the more than high school education category in the 24 hours activity time courses.
